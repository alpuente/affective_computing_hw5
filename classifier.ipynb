{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import scipy.io as io\n",
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers='hw5_data_code/deap_features.mat'\n",
    "dataHeaders= io.loadmat(headers)\n",
    "\n",
    "dataFile = 'hw5_data_code/results_DEAP.mat'\n",
    "#dataFile= io.loadmat(dataFile)\n",
    "#fname=data['label','PID','image','tumorBorder','tumorMask'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "features_frame = np.zeros(shape=(1280, 6))\n",
    "print(features_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeaturesDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, data_file, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.features_frame = io.loadmat(data_file)#pd.read_csv(csv_file)\n",
    "        self.features_frame = np.zeros(shape=(1280, 6))\n",
    "        i = 0\n",
    "        #print(dataHeaders.shape)\n",
    "        #for line in dataHeaders:\n",
    "        subjects = dataHeaders['features']\n",
    "        # read each feature set for each video for this subject into the data array\n",
    "        for subject in subjects:\n",
    "            for feature in subject:\n",
    "                # get mean of means for EMG features\n",
    "                mean_emg = np.mean(feature['EMG_feats'], axis=0)\n",
    "                mean_emg = mean_emg[0]\n",
    "                #print(mean_emg)\n",
    "                self.features_frame[i][0] = mean_emg\n",
    "                # set GSR mean\n",
    "                #print(feature['GSR_feats'])\n",
    "                self.features_frame[i][1] = feature['GSR_feats'][0][3]\n",
    "                # set RES mean\n",
    "                self.features_frame[i][2] = feature['RES_feats'][0][0]\n",
    "                # set BVP mean\n",
    "                #print(\"BVP\")\n",
    "                #print(feature['BVP_feats'])\n",
    "                self.features_frame[i][3] = feature['BVP_feats'][0][0]\n",
    "                self.features_frame[i]\n",
    "                #print(feature['feedback']['felt_arousal'])\n",
    "                #print(feature['feedback']['felt_arousal'])\n",
    "                self.features_frame[i][4] = feature['feedback']['felt_arousal']\n",
    "                self.features_frame[i][5] = feature['feedback']['felt_valence']\n",
    "                i += 1\n",
    "        # convert to pytorch tensor\n",
    "        self.features_frame = torch.from_numpy(self.features_frame)\n",
    "        #self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        #img_name = os.path.join(self.root_dir,\n",
    "        #                        self.features_frame.iloc[idx, 0])\n",
    "        #image = io.imread(img_name)\n",
    "        feature_set = self.features_frame[idx][0:4]\n",
    "        arousal = self.features_frame[idx][4]\n",
    "        valence = self.features_frame[idx][5]\n",
    "        #landmarks = np.array([landmarks])\n",
    "        #landmarks = landmarks.astype('float').reshape(-1, 2)\n",
    "        #print(torch.tensor([arousal]))\n",
    "        sample = {'features': feature_set, 'arousal':torch.tensor([arousal]), 'valence': valence}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_dataset = FeaturesDataset(dataFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features_frame = io.loadmat(dataFile)#pd.read_csv(csv_file)\n",
    "#features_frame = np.zeros(shape=(1280, 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#our class must extend nn.Module\n",
    "class MyClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyClassifier,self).__init__()\n",
    "        #Our network consists of 3 layers. 1 input, 1 hidden and 1 output layer\n",
    "        #This applies Linear transformation to input data. \n",
    "        self.fc1 = nn.Linear(4,4)\n",
    "        \n",
    "        #This applies linear transformation to produce output data\n",
    "        self.fc2 = nn.Linear(4,1)\n",
    "        \n",
    "    #This must be implemented\n",
    "    def forward(self,x):\n",
    "        #Output of the first layer\n",
    "        x = self.fc1(x)\n",
    "        #Activation function is Relu. Feel free to experiment with this\n",
    "        x = F.tanh(x)\n",
    "        #This produces output\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "        \n",
    "    #This function takes an input and predicts the class, (0 or 1)        \n",
    "    def predict(self,x):\n",
    "        #Apply softmax to output. \n",
    "        #pred = F.softmax(self.forward(x))\n",
    "        pred = self.forward(x)#F.softmax(self.forward(x))\n",
    "        ans = []\n",
    "        #Pick the class with maximum weight\n",
    "        for t in pred:\n",
    "            if t[0]>t[1]:\n",
    "                ans.append(0)\n",
    "            else:\n",
    "                ans.append(1)\n",
    "        return torch.tensor(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the model        \n",
    "model = MyClassifier()\n",
    "#Define loss criterion\n",
    "criterion = torch.nn.L1Loss()#nn.CrossEntropyLoss()\n",
    "#Define the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 2.5491184255182744\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-95-b5384438b73a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m#print(y_pred)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'arousal'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/hw5env/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/hw5env/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "face_dataset = FeaturesDataset(dataFile)\n",
    "trainloader = torch.utils.data.DataLoader(face_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Define the loss\n",
    "#criterion = nn.NLLLoss()\n",
    "criterion = nn.L1Loss()\n",
    "# Optimizers require the parameters to optimize and a learning rate\n",
    "#optimizer = optim.SGD(model.parameters(), lr=0.003)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "epochs = 3000\n",
    "model = model.float()\n",
    "i = 0\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for samples in trainloader:\n",
    "        #print(sample['features'])\n",
    "        #print(arousal)\n",
    "        #print(felt)\n",
    "        # Flatten MNIST images into a 784 long vector\n",
    "        #images = images.view(images.shape[0], -1)\n",
    "        \n",
    "        # Training pass\n",
    "        #optimizer.zero_grad()\n",
    "        \n",
    "        #output = model(images)\n",
    "        #loss = criterion(output, labels)\n",
    "        #loss.backward()\n",
    "        #optimizer.step()\n",
    "        x = samples['features']\n",
    "        x = x.float()\n",
    "        y_pred = model.forward(x)\n",
    "        #print(np.array(sample['arousal']))\n",
    "        #print(torch.tensor([arousal]))\n",
    "        #print(y_pred)\n",
    "        loss = criterion(y_pred, samples['arousal'])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        i += 1\n",
    "        if i % 1000 == 0:\n",
    "            print(f\"Training loss: {running_loss/len(trainloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateF1Score(correctHigh, falsePositives, totalHigh):\n",
    "    precision = correctHigh / (correctHigh + falsePositives)\n",
    "    # r is the number of correct positive results divided by the number of all relevant samples (all samples that should have been identified as positive)\n",
    "    recall = correctHigh / totalHigh\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNet(model, trainloader, epochs, label):\n",
    "    # Define the loss\n",
    "    #criterion = nn.NLLLoss()\n",
    "    criterion = nn.L1Loss()\n",
    "    # Optimizers require the parameters to optimize and a learning rate\n",
    "    #optimizer = optim.SGD(model.parameters(), lr=0.003)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    #epochs = 3000\n",
    "    model = model.float()\n",
    "    i = 0\n",
    "    for e in range(epochs):\n",
    "        running_loss = 0\n",
    "        for samples in trainloader:\n",
    "            #print(sample['features'])\n",
    "            #print(arousal)\n",
    "            #print(felt)\n",
    "            # Flatten MNIST images into a 784 long vector\n",
    "            #images = images.view(images.shape[0], -1)\n",
    "\n",
    "            # Training pass\n",
    "            #optimizer.zero_grad()\n",
    "\n",
    "            #output = model(images)\n",
    "            #loss = criterion(output, labels)\n",
    "            #loss.backward()\n",
    "            #optimizer.step()\n",
    "            x = samples['features']\n",
    "            x = x.float()\n",
    "            y_pred = model.forward(x)\n",
    "            #print(np.array(sample['arousal']))\n",
    "            #print(torch.tensor([arousal]))\n",
    "            #print(y_pred)\n",
    "            if label == 'arousal':\n",
    "                loss = criterion(y_pred, samples['arousal'])\n",
    "            elif label == 'valence':\n",
    "                loss = criterion(y_pred, samples['valence'])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            i += 1\n",
    "            #if i % 1000 == 0:\n",
    "    print(f\"Training loss: {running_loss/len(trainloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testNet(model, trainloader, label):\n",
    "    # Define the loss\n",
    "    #criterion = nn.NLLLoss()\n",
    "    criterion = nn.L1Loss()\n",
    "    \n",
    "    # Define the loss\n",
    "    #criterion = nn.NLLLoss()\n",
    "    criterion = nn.L1Loss()\n",
    "    # Optimizers require the parameters to optimize and a learning rate\n",
    "    #optimizer = optim.SGD(model.parameters(), lr=0.003)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    epochs = 30\n",
    "    model = model.float()\n",
    "    # how many were correctly guessed as high arousal\n",
    "    correctHigh = 0\n",
    "    # how many were actually high\n",
    "    totalHigh = 0\n",
    "    # how many were guessed as high\n",
    "    falsePositives = 0\n",
    "    # how many were correctly guessed as low arousal\n",
    "    correctLow = 0\n",
    "    # how many were actually low\n",
    "    totalLow = 0\n",
    "    # how many were guessed as low\n",
    "    falseNegatives = 0\n",
    "    for samples in trainloader:\n",
    "        #print(sample['features'])\n",
    "        #print(arousal)\n",
    "        #print(felt)\n",
    "        # Flatten MNIST images into a 784 long vector\n",
    "        #images = images.view(images.shape[0], -1)\n",
    "\n",
    "        # Training pass\n",
    "        #optimizer.zero_grad()\n",
    "\n",
    "        #output = model(images)\n",
    "        #loss = criterion(output, labels)\n",
    "        #loss.backward()\n",
    "        #optimizer.step()\n",
    "        x = samples['features']\n",
    "        x = x.float()\n",
    "        y_pred = model.forward(x)\n",
    "        #print(\"y %f x %f\" %(y_pred, samples['arousal']))\n",
    "        print(samples['valence'])\n",
    "        if label == 'valence':\n",
    "            #print(samples['arousal'])\n",
    "            if samples['valence'] > 5:\n",
    "                totalHigh += 1\n",
    "                if y_pred > 5:\n",
    "                    correctHigh += 1\n",
    "            elif y_pred > 5:\n",
    "                falsePositives += 1\n",
    "            if samples['valence'] < 5:\n",
    "                totalLow += 1\n",
    "                if y_pred < 5:\n",
    "                    correctLow += 1\n",
    "            elif y_pred < 5:\n",
    "                falseNegatives += 1\n",
    "        else:\n",
    "            #print(samples['arousal'])\n",
    "            if samples['arousal'] > 5:\n",
    "                totalHigh += 1\n",
    "                if y_pred > 5:\n",
    "                    correctHigh += 1\n",
    "            elif y_pred > 5:\n",
    "                falsePositives += 1\n",
    "            if samples['arousal'] < 5:\n",
    "                totalLow += 1\n",
    "                if y_pred < 5:\n",
    "                    correctLow += 1\n",
    "            elif y_pred < 5:\n",
    "                falseNegatives += 1\n",
    "    print(\"correct high %d total high %d percentage correct %f false positives %d\" % (correctHigh, totalHigh, ((correctHigh / totalHigh) * 100), falsePositives))\n",
    "    print(\"correct negative %d total low %d percentage correct %f false negatives %d\" % (correctLow, totalLow, ((correctLow / totalLow) * 100), falseNegatives))\n",
    "\n",
    "    print(\"f1 arousal %f\" %(calculateF1Score(correctHigh, falsePositives, totalHigh)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 3.8358828237876272\n",
      "tensor([1.9900, 5.0400, 6.9000, 1.0400, 1.9600, 1.0000, 6.0400, 4.0400, 1.0100,\n",
      "        3.9900, 4.9500, 6.9400, 4.6200, 6.0300, 4.0500, 6.3600, 7.0100, 6.7200,\n",
      "        4.9900, 7.9700, 7.0300, 6.9900, 6.1400, 2.0300, 7.4400, 3.9100, 8.0600,\n",
      "        1.0000, 5.1500, 5.0400, 6.8700, 7.0400, 4.1200, 3.1200, 5.0400, 9.0000,\n",
      "        6.0000, 6.0300, 1.2100, 2.5100, 3.4900, 8.9000, 4.9600, 4.0400, 2.0800,\n",
      "        8.0800, 5.1500, 7.0900, 4.0600, 6.8200, 4.1800, 7.5600, 6.8100, 3.1900,\n",
      "        6.1300, 7.6900, 4.5300, 4.9700, 6.9100, 4.0900, 2.9900, 6.0600, 5.9700,\n",
      "        3.1000], dtype=torch.float64)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "bool value of Tensor with more than one value is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-104-d94e96be944d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrainloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mface_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrainNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'valence'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtestNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'valence'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#trainNet(model, trainloader, 5000, 'arousal')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-103-16672ee85abf>\u001b[0m in \u001b[0;36mtestNet\u001b[0;34m(model, trainloader, label)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'valence'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;31m#print(samples['arousal'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'valence'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m                 \u001b[0mtotalHigh\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: bool value of Tensor with more than one value is ambiguous"
     ]
    }
   ],
   "source": [
    "FeaturesDataset(dataFile)\n",
    "trainloader = torch.utils.data.DataLoader(face_dataset, batch_size=64, shuffle=True)\n",
    "trainNet(model, trainloader, 1000, 'valence')\n",
    "testNet(model, trainloader, 'valence')\n",
    "# need to beat \n",
    "#trainNet(model, trainloader, 5000, 'arousal')\n",
    "#testNet(model, trainloader, 'arousal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct high 26 total high 708 percentage correct 3.672316 false positives 15\n",
      "correct negative 541 total low 556 percentage correct 97.302158 false negatives 698\n",
      "f1 arousal 0.069426\n"
     ]
    }
   ],
   "source": [
    "testNet(model, trainloader, 'valence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateF1Score(correctHigh, falsePositives, totalHigh):\n",
    "    precision = correctHigh / (correctHigh + falsePositives)\n",
    "    # r is the number of correct positive results divided by the number of all relevant samples (all samples that should have been identified as positive)\n",
    "    recall = correctHigh / totalHigh\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
